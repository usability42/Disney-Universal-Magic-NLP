{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940fb90a",
   "metadata": {},
   "source": [
    "# Project 3: Disney vs Universal, by Nadia Morgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace611fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae464c",
   "metadata": {},
   "source": [
    "# Step 1: Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db3cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54311816",
   "metadata": {},
   "source": [
    "Note: I had issues getting the data.  Sometimes it worked; sometimes it didn't.  First it worked for Disney, but not Universal.  Then I accidentally deleted the Disney results and couldn't get more than 100 rows after that, even though I used the same code.  Ugh.  If you get errors when you grade, it might not be my fault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54434ac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pt/67q6zqvx64n047rqdkj_wx5c0000gn/T/ipykernel_8071/3804839267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiz_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdisney\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpull_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'disneyparks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1631390454\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/pt/67q6zqvx64n047rqdkj_wx5c0000gn/T/ipykernel_8071/3804839267.py\u001b[0m in \u001b[0;36mpull_data\u001b[0;34m(subreddit, time_now)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# for loop to pull the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         res = requests.get(url, params = {\n\u001b[0m\u001b[1;32m     13\u001b[0m                                                  \u001b[0;34m'subreddit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                  \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def pull_data(subreddit, time_now):\n",
    "    \n",
    "    url = 'https://api.pushshift.io//reddit/search/submission?subreddit='\n",
    "    \n",
    "    diz_list = []\n",
    "    \n",
    "    # Setting epoch time\n",
    "    current_time = time_now\n",
    "    \n",
    "    # for loop to pull the data\n",
    "    for i in range(1, 52):\n",
    "        res = requests.get(url, params = {\n",
    "                                                 'subreddit':subreddit,\n",
    "                                                 'size':100,\n",
    "                                                 'lang': True,\n",
    "                                                 'before':current_time\n",
    "                                            })\n",
    "    \n",
    "        if res.status_code == 200:\n",
    "            # print i is for troubleshooting - grader might need it\n",
    "            # print(i)\n",
    "            \n",
    "            # Set up dataframe\n",
    "            df = pd.DataFrame(res.json()['data'])\n",
    "            df = df.loc[:,['title','selftext', 'created_utc','subreddit']]\n",
    "            diz_list.append(df)\n",
    "\n",
    "            # Time delay            \n",
    "            time.sleep(.3)\n",
    "            current_time = df['created_utc'].min()\n",
    "\n",
    "        elif 'df' in locals():\n",
    "            return df\n",
    "        else:\n",
    "            return 'Request Failed'\n",
    "        \n",
    "                    \n",
    "    return pd.concat(diz_list, axis= 0)\n",
    "\n",
    "disney = pull_data('disneyparks', 1631390454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a091022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for Universal\n",
    "# Note to grader: I had some issues with pulling the data for universal, even though I used the same\n",
    "# function as I did for Disney, but with the universal subreddit and different list name.\n",
    "# For reasons unknown to me and Eric, I couln't get more than about 52 iterations, and the number\n",
    "# of successful iterations differed each time I ran the exact same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153b4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pull_data_uni(subreddit, time_now):\n",
    "    \n",
    "    url = 'https://api.pushshift.io//reddit/search/submission?subreddit='\n",
    "    \n",
    "    univ_list = []\n",
    "    \n",
    "    # Setting epoch time\n",
    "    current_time = time_now\n",
    "    \n",
    "    # for loop to pull the data\n",
    "    for i in range(1, 52):\n",
    "        res = requests.get(url, params = {\n",
    "                                                 'subreddit':subreddit,\n",
    "                                                 'size':100,\n",
    "                                                 'lang': True,\n",
    "                                                 'before':current_time\n",
    "                                            })\n",
    "    \n",
    "        if res.status_code == 200:\n",
    "            print(i)\n",
    "            \n",
    "            # Set up dataframe\n",
    "            df2 = pd.DataFrame(res.json()['data'])\n",
    "            df2 = df2.loc[:,['title','selftext', 'created_utc','subreddit']]\n",
    "            univ_list.append(df2)\n",
    "\n",
    "            # Time delay            \n",
    "            time.sleep(.3)\n",
    "            current_time = df2['created_utc'].min()\n",
    "\n",
    "        elif 'df2' in locals():\n",
    "            return df2\n",
    "        else:\n",
    "            return 'Request Failed'\n",
    "        \n",
    "                    \n",
    "    return pd.concat(univ_list, axis= 0)\n",
    "\n",
    "universal = pull_data_uni('universalstudios', 1631390454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b32d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataframes\n",
    "df = pd.concat([disney, universal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68e5d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's better to load the data using this code:\n",
    "df = pd.read_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373ae7b",
   "metadata": {},
   "source": [
    "# Step 2:  Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make title and selftext lowercase\n",
    "\n",
    "df['title'] = df.title.str.lower()\n",
    "df['selftext'] = df.selftext.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and giveaway words, such as \"Disney\"\n",
    "# https://www.datasciencelearner.com/custom-stopwords-python-nlp/\n",
    "\n",
    "nltk.download('stopwords')\n",
    "#stop_words_list = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom stop words\n",
    "custom = ['disney', 'universal', \n",
    " 'animal kingdom', 'magic kingdom', 'hollywood studios', 'epcot', 'anaheim', 'eurodisney', \n",
    " 'typhoon lagoon', 'blizzard beach', 'wdw', 'main street', 'disneyland', 'preferred pass', \n",
    " 'city walk', 'seasonal pass', 'power pass', 'preferred pass', 'premier pass', 'volcano bay',\n",
    " 'islands of adventure', 'express pass', 'express unlimited', 'moscow', 'beijing', 'singapore',\n",
    " 'mickey', 'cruise', 'incredi-pass', 'sorcerer pass', 'pirate pass', 'pixie dust pass', \n",
    " 'platinum pass', 'gold pass', 'silver pass', 'bronze pass', 'walt', 'land paris', 'tokyo land',\n",
    " 'park hopper', 'studios hollywood', 'studios orlando', 'land']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stopwords = custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7150ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement stop words\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79a8331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had a LOT of trouble pulling my data; I'm backing up everything!\n",
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2f553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify the backup is there\n",
    "df_backup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85c3b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since only 21 posts have missing selftext, I'm dropping those 21\n",
    "df.dropna(subset=['selftext'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "\n",
    "bye_stopwords = '|'.join(final_stopwords)\n",
    "df['title_clean'] = df.title.replace(bye_stopwords,'', regex=True)\n",
    "df['selftext_clean'] = df.selftext.replace(bye_stopwords,'', regex=True)\n",
    "\n",
    "# https://stackoverflow.com/questions/66129517/remove-custom-stop-words-from-pandas-dataframe-not-working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e94a1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize title and selftext\n",
    "\n",
    "df['title_clean'] = df.apply(lambda row: nltk.word_tokenize(row['title_clean']), axis=1)\n",
    "df['selftext_clean'] = df.apply(lambda row: nltk.word_tokenize(row['selftext_clean']), axis=1)\n",
    "\n",
    "# https://stackoverflow.com/questions/52869456/how-to-use-word-tokenize-on-a-single-column-in-a-data-frame-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018561f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>clean_lemon_tokens</th>\n",
       "      <th>cln_lmn_tok_title</th>\n",
       "      <th>disney_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>anybody have this 2099 expiration issue with n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631369894</td>\n",
       "      <td>[[, 'anybody, ', ,, 'have, ', ,, 'this, ', ,, ...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['anybody', 'have', 'this', '2099', 'expiratio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i went to wdw back in april, here are some pic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631367728</td>\n",
       "      <td>[[, ', i, ', ,, 'went, ', ,, 'to, ', ,, 'back,...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['i', 'went', 'to', 'back', 'in', 'april', ','...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>full walkthrough of villain's grove at opening...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631339851</td>\n",
       "      <td>[[, 'full, ', ,, 'walkthrough, ', ,, 'of, ', ,...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['full', 'walkthrough', 'of', 'villain', \"'s\",...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>one of my favorite pieces of park merchandise ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631339788</td>\n",
       "      <td>[[, 'one, ', ,, 'of, ', ,, 'my, ', ,, 'favorit...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['one', 'of', 'my', 'favorite', 'piece', 'of',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>first time at avengers campus and rode incredi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631337412</td>\n",
       "      <td>[[, 'first, ', ,, 'time, ', ,, 'at, ', ,, 'ave...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['first', 'time', 'at', 'avenger', 'campus', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>university study on visiting the disney theme ...</td>\n",
       "      <td>hi! we’re a group of researchers at texas tech...</td>\n",
       "      <td>1631312443</td>\n",
       "      <td>[[, 'university, ', ,, 'study, ', ,, 'on, ', ,...</td>\n",
       "      <td>[[, 'hi, ', ,, ', !, ', ,, 'we, ', ,, ', ’, ',...</td>\n",
       "      <td>['hi', '!', 'we', '’', 're', 'a', 'group', 'of...</td>\n",
       "      <td>['university', 'study', 'on', 'visiting', 'the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>university study on visiting the disney theme ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1631311774</td>\n",
       "      <td>[[, 'university, ', ,, 'study, ', ,, 'on, ', ,...</td>\n",
       "      <td>[[, ', [, ', ,, 'removed, ', ,, ', ], ', ]]</td>\n",
       "      <td>['[', 'removed', ']']</td>\n",
       "      <td>['university', 'study', 'on', 'visiting', 'the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>“we believe in our idea: a family park where p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631307042</td>\n",
       "      <td>[[, ', “, ', ,, 'we, ', ,, 'believe, ', ,, 'in...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['“', 'we', 'believe', 'in', 'our', 'idea', ':...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>when you think about it, epcot forever is a pr...</td>\n",
       "      <td>it's a wonderful bunch of beloved original, cr...</td>\n",
       "      <td>1631303562</td>\n",
       "      <td>[[, 'when, ', ,, 'you, ', ,, 'think, ', ,, 'ab...</td>\n",
       "      <td>[[, 'it, ', ,, ``, 's, '', ,, ', a, ', ,, 'won...</td>\n",
       "      <td>['it', \"'s\", 'a', 'wonderful', 'bunch', 'of', ...</td>\n",
       "      <td>['when', 'you', 'think', 'about', 'it', ',', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>bring fritters back to disneyland!!! the ones ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631245797</td>\n",
       "      <td>[[, 'bring, ', ,, 'fritters, ', ,, 'back, ', ,...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['bring', 'fritter', 'back', 'to', '!', '!', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  anybody have this 2099 expiration issue with n...   \n",
       "1           1  i went to wdw back in april, here are some pic...   \n",
       "2           2  full walkthrough of villain's grove at opening...   \n",
       "3           3  one of my favorite pieces of park merchandise ...   \n",
       "4           4  first time at avengers campus and rode incredi...   \n",
       "5           5  university study on visiting the disney theme ...   \n",
       "6           6  university study on visiting the disney theme ...   \n",
       "7           7  “we believe in our idea: a family park where p...   \n",
       "8           8  when you think about it, epcot forever is a pr...   \n",
       "9           9  bring fritters back to disneyland!!! the ones ...   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "0                                                NaN   1631369894   \n",
       "1                                                NaN   1631367728   \n",
       "2                                                NaN   1631339851   \n",
       "3                                                NaN   1631339788   \n",
       "4                                                NaN   1631337412   \n",
       "5  hi! we’re a group of researchers at texas tech...   1631312443   \n",
       "6                                          [removed]   1631311774   \n",
       "7                                                NaN   1631307042   \n",
       "8  it's a wonderful bunch of beloved original, cr...   1631303562   \n",
       "9                                                NaN   1631245797   \n",
       "\n",
       "                                         title_clean  \\\n",
       "0  [[, 'anybody, ', ,, 'have, ', ,, 'this, ', ,, ...   \n",
       "1  [[, ', i, ', ,, 'went, ', ,, 'to, ', ,, 'back,...   \n",
       "2  [[, 'full, ', ,, 'walkthrough, ', ,, 'of, ', ,...   \n",
       "3  [[, 'one, ', ,, 'of, ', ,, 'my, ', ,, 'favorit...   \n",
       "4  [[, 'first, ', ,, 'time, ', ,, 'at, ', ,, 'ave...   \n",
       "5  [[, 'university, ', ,, 'study, ', ,, 'on, ', ,...   \n",
       "6  [[, 'university, ', ,, 'study, ', ,, 'on, ', ,...   \n",
       "7  [[, ', “, ', ,, 'we, ', ,, 'believe, ', ,, 'in...   \n",
       "8  [[, 'when, ', ,, 'you, ', ,, 'think, ', ,, 'ab...   \n",
       "9  [[, 'bring, ', ,, 'fritters, ', ,, 'back, ', ,...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0                                             [[, ]]   \n",
       "1                                             [[, ]]   \n",
       "2                                             [[, ]]   \n",
       "3                                             [[, ]]   \n",
       "4                                             [[, ]]   \n",
       "5  [[, 'hi, ', ,, ', !, ', ,, 'we, ', ,, ', ’, ',...   \n",
       "6        [[, ', [, ', ,, 'removed, ', ,, ', ], ', ]]   \n",
       "7                                             [[, ]]   \n",
       "8  [[, 'it, ', ,, ``, 's, '', ,, ', a, ', ,, 'won...   \n",
       "9                                             [[, ]]   \n",
       "\n",
       "                                  clean_lemon_tokens  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "5  ['hi', '!', 'we', '’', 're', 'a', 'group', 'of...   \n",
       "6                              ['[', 'removed', ']']   \n",
       "7                                                 []   \n",
       "8  ['it', \"'s\", 'a', 'wonderful', 'bunch', 'of', ...   \n",
       "9                                                 []   \n",
       "\n",
       "                                   cln_lmn_tok_title  disney_y  \n",
       "0  ['anybody', 'have', 'this', '2099', 'expiratio...         1  \n",
       "1  ['i', 'went', 'to', 'back', 'in', 'april', ','...         1  \n",
       "2  ['full', 'walkthrough', 'of', 'villain', \"'s\",...         1  \n",
       "3  ['one', 'of', 'my', 'favorite', 'piece', 'of',...         1  \n",
       "4  ['first', 'time', 'at', 'avenger', 'campus', '...         1  \n",
       "5  ['university', 'study', 'on', 'visiting', 'the...         1  \n",
       "6  ['university', 'study', 'on', 'visiting', 'the...         1  \n",
       "7  ['“', 'we', 'believe', 'in', 'our', 'idea', ':...         1  \n",
       "8  ['when', 'you', 'think', 'about', 'it', ',', '...         1  \n",
       "9  ['bring', 'fritter', 'back', 'to', '!', '!', '...         1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying that data is tokenized\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_=[]\n",
    "\n",
    "for row in df['selftext_clean']:\n",
    "    lem_row = [lemmatizer.lemmatize(string) for string in row]\n",
    "    list_.append(lem_row)\n",
    "df[f'clean_lemon_tokens'] = list_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0437064",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_title=[]\n",
    "\n",
    "for row in df['title_clean']:\n",
    "    lem_row = [lemmatizer.lemmatize(string) for string in row]\n",
    "    list_title.append(lem_row)\n",
    "df[f'cln_lmn_tok_title'] = list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify it worked\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummmify subreddit: 1 = Disney, 0 = Universal\n",
    "# Note: I didn't use drop_first, because I wanted Disney to appear in the dataframe and for\n",
    "# Disney to equal 1 and Universal 0, since I'm pitching to Disney\n",
    "\n",
    "df = pd.get_dummies(df, columns=['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify it worked\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f583dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate dummy column\n",
    "df.drop(columns=['subreddit_universalstudios', 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'subreddit_disneyparks':'disney_y'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b851261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>clean_lemon_tokens</th>\n",
       "      <th>cln_lmn_tok_title</th>\n",
       "      <th>disney_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anybody have this 2099 expiration issue with n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1631369894</td>\n",
       "      <td>[[, 'anybody, ', ,, 'have, ', ,, 'this, ', ,, ...</td>\n",
       "      <td>[[, ]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['anybody', 'have', 'this', '2099', 'expiratio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title selftext  created_utc  \\\n",
       "0  anybody have this 2099 expiration issue with n...      NaN   1631369894   \n",
       "\n",
       "                                         title_clean selftext_clean  \\\n",
       "0  [[, 'anybody, ', ,, 'have, ', ,, 'this, ', ,, ...         [[, ]]   \n",
       "\n",
       "  clean_lemon_tokens                                  cln_lmn_tok_title  \\\n",
       "0                 []  ['anybody', 'have', 'this', '2099', 'expiratio...   \n",
       "\n",
       "   disney_y  \n",
       "0         1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3aa5c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selftext field has a lot more missing values than isna counted\n",
    "# Copying to df2 for future analysis on selftext\n",
    "\n",
    "df2 = df.copy()\n",
    "df2.dropna(subset=['selftext'], inplace=True)\n",
    "df2.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72228d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3911, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a44d7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename({'subreddit_disneyparks':'disney_y'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55d70e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>clean_lemon_tokens</th>\n",
       "      <th>cln_lmn_tok_title</th>\n",
       "      <th>disney_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>university study on visiting the disney theme ...</td>\n",
       "      <td>hi! we’re a group of researchers at texas tech...</td>\n",
       "      <td>1631312443</td>\n",
       "      <td>['[', \"'university\", \"'\", ',', \"'study\", \"'\", ...</td>\n",
       "      <td>['[', \"'hi\", \"'\", ',', \"'\", '!', \"'\", ',', \"'w...</td>\n",
       "      <td>['hi', '!', 'we', '’', 're', 'a', 'group', 'of...</td>\n",
       "      <td>['university', 'study', 'on', 'visiting', 'the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                                              title  \\\n",
       "5             5  university study on visiting the disney theme ...   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "5  hi! we’re a group of researchers at texas tech...   1631312443   \n",
       "\n",
       "                                         title_clean  \\\n",
       "5  ['[', \"'university\", \"'\", ',', \"'study\", \"'\", ...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "5  ['[', \"'hi\", \"'\", ',', \"'\", '!', \"'\", ',', \"'w...   \n",
       "\n",
       "                                  clean_lemon_tokens  \\\n",
       "5  ['hi', '!', 'we', '’', 're', 'a', 'group', 'of...   \n",
       "\n",
       "                                   cln_lmn_tok_title  disney_y  \n",
       "5  ['university', 'study', 'on', 'visiting', 'the...         1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7cfa354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5093"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of disneyparks subreddit posts in complete dataframe (n = 10,177) - for title\n",
    "df['disney_y'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b0b72aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5084"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of universalstudios subreddit posts in complete dataframe (n = 10,177) - for title\n",
    "10177 - 5093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3160584c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of disneyparks subreddit posts in small dataframe (n = 3,911) - for selftext\n",
    "df2['disney_y'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b766a474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2498"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of universalstudios subreddit posts in small dataframe (n = 3,911) - for selftext\n",
    "3911 - 1413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec701b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_finally_clean.csv', index=True)\n",
    "df2.to_csv('df2_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c177d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
